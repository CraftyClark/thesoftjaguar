<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The soft jaguar</title><link>http://thesoftjaguar.com/</link><description></description><atom:link href="http://thesoftjaguar.com/feeds/scale.rss.xml" rel="self"></atom:link><lastBuildDate>Tue, 30 Jul 2013 09:51:00 +0200</lastBuildDate><item><title>Berlin Buzzwords 2013</title><link>http://thesoftjaguar.com/posts/2013/07/30/berlin-buzzwords-2013/</link><description>&lt;p&gt;&lt;em&gt;Berlin Buzzwords&lt;/em&gt; took place on June 3rd and 4th at &lt;a href="http://kulturbrauerei.de/en"&gt;Kulturbrauerei&lt;/a&gt;, which is an awesome place. &lt;em&gt;Buzzwords&lt;/em&gt; is a conference focused in Open Source technologies, mainly related to search engines, &lt;em&gt;Big Data&lt;/em&gt; and &lt;em&gt;NoSQL&lt;/em&gt; databases, having three main topics: &lt;code&gt;search&lt;/code&gt;, &lt;code&gt;store&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt;. As in FrosCon, ApacheCon or FOSDEM, I went with my colleagues from &lt;a href="http://edelight.de"&gt;edelight&lt;/a&gt;, which sponsored the trip as usual (and is always proposing new events).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Maschinenhaus" src="http://i.imgur.com/cU6iwfG.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;The main technologies from the talks that I attended were &lt;a href="http://www.elasticsearch.org/"&gt;Elasticsearch&lt;/a&gt;, &lt;a href="http://lucene.apache.org/solr/"&gt;Solr&lt;/a&gt; &lt;a href="http://cassandra.apache.org/"&gt;Cassandra&lt;/a&gt;, &lt;a href="http://hadoop.apache.org/"&gt;Hadoop&lt;/a&gt;, &lt;a href="http://pig.apache.org/"&gt;Pig&lt;/a&gt;, &lt;a href="http://mahout.apache.org/"&gt;Mahout&lt;/a&gt;, &lt;a href="http://fitnesse.org/"&gt;FitNesse&lt;/a&gt;, &lt;a href="http://www.mongodb.org/"&gt;MongoDB&lt;/a&gt;, &lt;a href="http://logstash.net/"&gt;Logstash&lt;/a&gt; and &lt;a href="http://kibana.org/"&gt;Kibana&lt;/a&gt;, among others.&lt;/p&gt;
&lt;p&gt;Instead of explaining what was going on in each talk that I attended, I have decided to cover the three main areas of the conference.&lt;/p&gt;
&lt;h1&gt;Search&lt;/h1&gt;
&lt;p&gt;This topic covers several search engines, mainly &lt;em&gt;Elasticsearch&lt;/em&gt;, but also &lt;em&gt;Solr&lt;/em&gt; (and of course &lt;em&gt;Lucene&lt;/em&gt; as the base), which are quite famous. But it also wraps how to analyze a lot of log information (or just a lot of data). I mainly attended &lt;em&gt;Elasticsearch&lt;/em&gt; talks, but there were more technologies involved.&lt;/p&gt;
&lt;h2&gt;Just search&lt;/h2&gt;
&lt;p&gt;So we want to get started with a search engine, but... How do we use them? &lt;a href="http://berlinbuzzwords.de/sessions/getting-down-and-dirty-elasticsearch"&gt;Getting down and dirty with Elasticsearch&lt;/a&gt; starts from the basic concepts and explains how to improve our queries (which, IMHO, is the most interesting part of this talk). It differences exact values search (should match &lt;strong&gt;entirely&lt;/strong&gt;) from full text search (search within the text), it introduces the &lt;em&gt;inverted index&lt;/em&gt; concept for improving full text search, which is being done separating words and terms, sorting unique terms and listing docs containing those terms, via the use of &lt;strong&gt;analyzers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The exact matching is done (or should be done) with &lt;strong&gt;filters&lt;/strong&gt;, and the text search is achieved with &lt;strong&gt;queries&lt;/strong&gt;, so the filters are faster than the queries and cacheable, but the queries provide the full text search. Finally, the talk addresses some different ways of implementing &lt;strong&gt;autocomplete&lt;/strong&gt;: the &lt;em&gt;N-grams&lt;/em&gt; method, good for partial word matching, and the &lt;em&gt;Edge N-grams&lt;/em&gt; method, perfect for autocomplete (just activating the type &lt;code&gt;edge_ngram&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;analyzers&lt;/strong&gt; is a very interesting topic, helping us to deal with different languages at query time. &lt;a href="http://berlinbuzzwords.de/sessions/language-support-and-linguistics-lucenesolrelasticsearch-and-open-source-and-commercial-eco"&gt;Language Support and Linguistics in Lucene/Solr/Elasticsearch and the open source ecosystem&lt;/a&gt; explains again the tokenization and normalization of the given text query, where the tokens are mapped to the document ids that contain them.&lt;/p&gt;
&lt;p&gt;However, we will find the &lt;a href="http://en.wikipedia.org/wiki/Precision_and_recall"&gt;precision and recall&lt;/a&gt; problem. The talk explains how Lucene, Elasticsearch and Solr deals with this, and how the synonyms can improve the recall. About the latter, the best practice is to apply the synonyms in the query side instead of in the index: it allows synonym updating without reindexing and is easier to turn the synonym feature off.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kesselhaus" src="http://i.imgur.com/pWprsej.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;When we work with &lt;em&gt;NoSQL&lt;/em&gt; based solutions, in this case with Elasticsearch, we always have a problem: how to divide the relational data? &lt;a href="http://berlinbuzzwords.de/sessions/document-relations-elasticsearch"&gt;Document relations with Elasticsearch&lt;/a&gt; gives two answers to this problem. The first one is about setting the &lt;code&gt;_parent&lt;/code&gt; field in the desired mapping, for linking the current document (&lt;em&gt;child&lt;/em&gt;) to another one (&lt;em&gt;parent&lt;/em&gt;). The advantage is that the parent document doesn't need to exist at indexing time, which improves the performance, and if we want to have the parent documents based on matches with their child ones, we can always set the &lt;code&gt;has_child&lt;/code&gt; query.&lt;/p&gt;
&lt;p&gt;The second workaround is the use of &lt;strong&gt;nested objects&lt;/strong&gt;. We can set a JSON document with nested fields, instead of defining a parent, using the &lt;code&gt;nested&lt;/code&gt; field type (which triggers Lucene's &lt;em&gt;block indexing&lt;/em&gt;). This document will be flattened and the &lt;em&gt;block indexing&lt;/em&gt; translate the ES document into multiple Lucene documents. With this approach, the root document and its nested documents remain always in the same block, and when querying, you establish it as a nested query, specify the nested level, the score mode and then the query inside that nested document.&lt;/p&gt;
&lt;h2&gt;Test driven development in Big Data&lt;/h2&gt;
&lt;p&gt;The search topic is also closely related to Big Data. What's the point of having tons and tons of data if we can't find anything useful there? Big Data solutions usually are complex and not easy to test.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://berlinbuzzwords.de/sessions/bug-bites-elephant-test-driven-quality-assurance-big-data-application-development"&gt;Bug bites Elephant?&lt;/a&gt; presents a way of assuring quality data following the Test-driven development process, specifically when using &lt;em&gt;Hadoop&lt;/em&gt;, &lt;em&gt;Pig&lt;/em&gt; and/or &lt;em&gt;Hive&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;There are several ways of achieving this (&lt;em&gt;JUnit&lt;/em&gt;, &lt;em&gt;MRUnit&lt;/em&gt;, &lt;em&gt;iTest&lt;/em&gt; or simply using scripts), but the talk introduces &lt;em&gt;FitNesse&lt;/em&gt; as a natural language test specification, where the tests are written as stories instead as programming code. The FitNesse server translates the natural language into Java and integrates with REST or Jenkins if needed.&lt;/p&gt;
&lt;p&gt;&lt;img alt="FitNesse into action" src="http://i.imgur.com/O1Qfp1o.png" /&gt;&lt;/p&gt;
&lt;p&gt;This has several advantages, like having a wiki page with the tests written in a language that everyone can understand (or integrate PigLatin directly into that wiki page). However, natural language has its limits, like for instance, if you want to check the expected result (like the output of a Pig job alias).&lt;/p&gt;
&lt;h2&gt;Don't drown in a log ocean&lt;/h2&gt;
&lt;p&gt;Who hasn't had issues when dealing with log information? We want to know what's going on when something is wrong, but sometimes the log is too verbose (Java :P) or simply we have the log files distributed among a lot of servers.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Big Data is any thing which is crash Excel" src="http://i.imgur.com/3tfClkO.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://berlinbuzzwords.de/sessions/state-open-source-logging"&gt;The State of Open Source Logging&lt;/a&gt; shows some technologies that address this problem, like &lt;em&gt;fluentd&lt;/em&gt;, &lt;em&gt;Logstash&lt;/em&gt;, &lt;em&gt;Graylog2&lt;/em&gt;, &lt;em&gt;ELSA&lt;/em&gt;, &lt;em&gt;Flume&lt;/em&gt; or &lt;em&gt;Scribe&lt;/em&gt;. Therefore, one work around to this problem is to centralize all log files into a Elasticsearch cluster, transforming every log line into a JSON document. The talk is focused on the &lt;strong&gt;Kibana&lt;/strong&gt; way, which is a very powerful Logstash and Elasticsearch interface.&lt;/p&gt;
&lt;h2&gt;Machine learning baby&lt;/h2&gt;
&lt;p&gt;Imagine that we have access to a lot of data, and actually we can (Twitter Public API). Now we want to do something useful with that data, like in &lt;a href="http://berlinbuzzwords.de/sessions/geospatial-event-detection-twitter-stream"&gt;Geospatial Event Detection in the Twitter Stream&lt;/a&gt;, a nice talk stating the desire of creating real actionable insights from tweet data: fires, police alerts, events, demonstrations, accidents...&lt;/p&gt;
&lt;p&gt;&lt;img alt="There's a fire! Twitter knows before anyone" src="http://i.imgur.com/IOV0C27.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;In order to detect those events, they grouped the tweets by location (tweet cluster) storing them in &lt;em&gt;MongoDB&lt;/em&gt;, and then, check if the tweets from the cluster belong to the same topic (marking the group as good). Where is the machine learning? Well, marking the tweet cluster as good is not a trivial thing: the use of a machine learning tool (like Weka) can solve the problem.&lt;/p&gt;
&lt;p&gt;The machine learning tool will then make a choice (is the cluster good or bad?) based on a series of user defined rules. In this specific case, it was checking if the tweets had a common theme (n-gram overlap), sentiment (was it positive, negative? Which was the overall sentiment? What would be the sentiment strenght?), subjectivity, number of hashtags, retweet ratio, event categories, embedded links, foursquare tweets (or other kind of predefined tweets), total number of tweets in the cluster, unique locations, bad locations (airports, train stations...), among other parameters. This highlights the relevance of having an important number of quality rules, besides the tool used for performing machine learning.&lt;/p&gt;
&lt;h1&gt;Store&lt;/h1&gt;
&lt;h2&gt;Cassandra in da house&lt;/h2&gt;
&lt;p&gt;I didn't assist a lot of &lt;code&gt;Store&lt;/code&gt; talks, and the ones that I did were almost all about &lt;em&gt;Cassandra&lt;/em&gt;, topic in which I am not too familiar with. &lt;a href="http://berlinbuzzwords.de/sessions/cassandras-evolutions"&gt;On Cassandra's Evolution&lt;/a&gt; gave a brief explanation about Casandra's ring of nodes.&lt;/p&gt;
&lt;p&gt;They explain the concept of &lt;em&gt;token&lt;/em&gt;. A &lt;em&gt;token&lt;/em&gt; belongs to a data range, so the entire data will be divided in &lt;em&gt;tokens&lt;/em&gt;. So how is the data going to be distributed using these tokens? There are two ways: without and with virtual nodes. The main difference is that virtual nodes will use more tokens per node, (smaller ones)&lt;/p&gt;
&lt;p&gt;&lt;img alt="Virtual Nodes in Cassandra" src="http://www.datastax.com/wp-content/uploads/2012/10/VNodes3.png" /&gt;&lt;/p&gt;
&lt;p&gt;Other concepts were explained, like how to repair the cluster with and without virtual nodes, but at the end, the use of virtual nodes was more interesting: you can pick tokens from pretty much every node in the ring, instead of from some nodes of your entire cluster (this is due to the fact that the tokens are smaller), so if you have a lot of data to transfer because you are rebuilding a node, this can make the difference. There are other advantages, they allow heterogeneous nodes and the load balancing is simpler when adding new nodes.&lt;/p&gt;
&lt;p&gt;Another interesting part was the introduction to the &lt;strong&gt;Cassandra Query Language&lt;/strong&gt; (CQL3). Is kind of a &lt;em&gt;denormalized&lt;/em&gt; SQL, strictly real time oriented, with no joins, no sub-queries, no aggregation and a limited ORDER BY. They also announced the replace of the Thrift transport protocol for a binary (native) one, which is asynchronous, gives server notifications for new nodes and schema changes, and is totally optimized for CQL3. Another interesting concept is the request tracing, you can trace queries, for instance, seeing which node receives the query and which nodes has the requires replicas, making debugging easier for tracing anti patterns.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Request tracing" src="http://i.imgur.com/6AcXIX8.png" /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href="http://berlinbuzzwords.de/sessions/cassandra-example-data-modeling-cql3"&gt;Cassandra by Example&lt;/a&gt; they extend these concepts using a Django example application called &lt;a href="https://github.com/twissandra/twissandra"&gt;Twissandra&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Scale&lt;/h1&gt;
&lt;p&gt;The most relevant talk about &lt;code&gt;scale&lt;/code&gt;, in my opinion, was &lt;a href="http://berlinbuzzwords.de/sessions/scaling-other-way-elasticsearch-miniature"&gt;Elasticsearch in Miniature&lt;/a&gt;, a nice way of presenting Elasticsearch's distributed capabilities.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Master not found... Doh!" src="http://i.imgur.com/8eW4m23.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;The cool thing is that the Elasticsearch guys installed their system in 5 Raspberry Pi, creating a test ES cluster over WiFi: this allowed them to &lt;a href="http://www.youtube.com/watch?feature=player_embedded&amp;amp;v=AA_gihv5H-Y"&gt;show us&lt;/a&gt; things like how the shards and replicas were rebalanced when the number of nodes in the cluster was changing, or what was the cluster state after destroying and creating replicas. The interesting thing is that everything went fine, having into account that this kind of demos usually tend to fail in the final presentation, and the fact that the network wasn't really reliable is a plus.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dario Blanco</dc:creator><pubDate>Tue, 30 Jul 2013 09:51:00 +0200</pubDate><guid>tag:thesoftjaguar.com,2013-07-30:posts/2013/07/30/berlin-buzzwords-2013/</guid><category>conferences</category><category>elasticsearch</category><category>solr</category><category>lucene</category><category>open-source</category><category>big-data</category><category>java</category><category>cassandra</category><category>hadoop</category><category>pig</category><category>search</category><category>store</category><category>scale</category></item></channel></rss>