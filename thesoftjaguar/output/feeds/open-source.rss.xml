<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The soft jaguar</title><link>http://thesoftjaguar.com/</link><description></description><atom:link href="http://thesoftjaguar.com/feeds/open-source.rss.xml" rel="self"></atom:link><lastBuildDate>Tue, 30 Jul 2013 09:51:00 +0200</lastBuildDate><item><title>Berlin Buzzwords 2013</title><link>http://thesoftjaguar.com/posts/2013/07/30/berlin-buzzwords-2013/</link><description>&lt;p&gt;Berlin Buzzwords is another of those events that I like to attend, it took place on June 3rd and 4th at &lt;a href="http://kulturbrauerei.de/en"&gt;Kulturbrauerei&lt;/a&gt;, which is an awesome place.&lt;/p&gt;
&lt;p&gt;Buzzwords is a conference focused in Open Source technologies, mainly related to search engines, big data and NoSQL databases, having three main topics: &lt;code&gt;search&lt;/code&gt;, &lt;code&gt;store&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt;. The main technologies from the talks that I attended were &lt;a href="http://www.elasticsearch.org/"&gt;Elasticsearch&lt;/a&gt;, &lt;a href="http://lucene.apache.org/solr/"&gt;Solr&lt;/a&gt; &lt;a href="http://cassandra.apache.org/"&gt;Cassandra&lt;/a&gt;, &lt;a href="http://hadoop.apache.org/"&gt;Hadoop&lt;/a&gt;, &lt;a href="http://pig.apache.org/"&gt;Pig&lt;/a&gt;, &lt;img alt="Mahout" src="http://mahout.apache.org/" /&gt;, &lt;a href="http://fitnesse.org/"&gt;FitNesse&lt;/a&gt;, &lt;a href="http://www.mongodb.org/"&gt;MongoDB&lt;/a&gt;, &lt;a href="http://logstash.net/"&gt;Logstash&lt;/a&gt; and &lt;a href="http://kibana.org/"&gt;Kibana&lt;/a&gt;, among others.&lt;/p&gt;
&lt;p&gt;Instead of explaining what was going on in each talk that I attended, I have decided to cover the three main areas of the conference.&lt;/p&gt;
&lt;h1&gt;Search&lt;/h1&gt;
&lt;p&gt;This topic covers several search engines, mainly &lt;code&gt;Elasticsearch&lt;/code&gt;, but also &lt;code&gt;Solr&lt;/code&gt; (and of course &lt;code&gt;Lucene&lt;/code&gt; as the base), which are quite famous. But it also wraps how to analyze a lot of log information (or just a lot of data). I mainly attended Elasticsearch's talks, but there were more technologies involved.&lt;/p&gt;
&lt;h2&gt;Just search&lt;/h2&gt;
&lt;p&gt;So we want to get started with a search engine, but... How do we use them? &lt;a href="http://berlinbuzzwords.de/sessions/getting-down-and-dirty-elasticsearch"&gt;Getting down and dirty with Elasticsearch&lt;/a&gt; starts from the basic concepts and explains how to improve our queries (which, IMHO, is the most interesting part of this talk). It differences exact values search (should match &lt;strong&gt;entirely&lt;/strong&gt;) from full text search (search within the text), it introduces the &lt;em&gt;inverted index&lt;/em&gt; concept for improving full text search, which is being done separating words and terms, sorting unique terms and listing docs containing those terms, via the use of &lt;strong&gt;analyzers&lt;/strong&gt;. The exact matching is done (or should be done) with &lt;strong&gt;filters&lt;/strong&gt;, and the text search is achieved with &lt;strong&gt;queries&lt;/strong&gt;, so the filters are faster than the queries and cacheable, but the queries provide the full text search. Finally, the talk addresses some different ways of implementing &lt;strong&gt;autocomplete&lt;/strong&gt;: the &lt;em&gt;N-grams&lt;/em&gt; method, good for partial word matching, and the &lt;em&gt;Edge N-grams&lt;/em&gt; method, perfect for autocomplete (just activating the type &lt;code&gt;edge_ngram&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;analyzers&lt;/strong&gt; is a very interesting topic, helping us to deal with different languages at query time. &lt;a href="http://berlinbuzzwords.de/sessions/language-support-and-linguistics-lucenesolrelasticsearch-and-open-source-and-commercial-eco"&gt;Language Support and Linguistics in Lucene/Solr/Elasticsearch and the open source ecosystem&lt;/a&gt; explains again the tokenization and normalization of the given text query, where the tokens are mapped to the document ids that contain them. However, we will find the &lt;a href="http://en.wikipedia.org/wiki/Precision_and_recall"&gt;precision and recall&lt;/a&gt; problem. The talk explains how Lucene, Elasticsearch and Solr deals with this, and how the synonyms can improve the recall. About the latter, the best practice is to apply the synonyms in the query side instead of in the index: it allows synonym updating without reindexing and is easier to turn the synonym feature off.&lt;/p&gt;
&lt;p&gt;When we work with NoSQL based solutions, in this case with Elasticsearch, we always have a problem: how to divide the relational data? &lt;a href="http://berlinbuzzwords.de/sessions/document-relations-elasticsearch"&gt;Document relations with Elasticsearch&lt;/a&gt; gives two answers to this problem. The first one is about setting the &lt;code&gt;_parent&lt;/code&gt; field in the desired mapping, for linking the current document (child) to another one (parent). The advantage is that the parent document doesn't need to exist at indexing time, which improves the performance, and if we want to have the parent documents based on matches with their child ones, we can always set the &lt;code&gt;has_child&lt;/code&gt; query. The second workaround is the use of &lt;strong&gt;nested objects&lt;/strong&gt;. We can set a JSON document with nested fields, instead of defining a parent, using the &lt;code&gt;nested&lt;/code&gt; field type (which triggers Lucene's &lt;em&gt;block indexing&lt;/em&gt;). This document will be flattened and the &lt;em&gt;block indexing&lt;/em&gt; translate the ES document into multiple Lucene documents. With this approach, the root document and its nested documents remain always in the same block, and when querying, you establish it as a nested query, specify the nested level, the score mode and then the query inside that nested document.&lt;/p&gt;
&lt;h2&gt;Test driven development in Big Data&lt;/h2&gt;
&lt;p&gt;The search topic is also closely related to Big Data. What's the point of having tons and tons of data if we can't find anything useful there? Big Data solutions usually are complex and not easy to test. &lt;a href="http://berlinbuzzwords.de/sessions/bug-bites-elephant-test-driven-quality-assurance-big-data-application-development"&gt;Bug bites Elephant?&lt;/a&gt; presents a way of assuring quality data following the Test-driven development process, specifically when using Hadoop, Pig and/or Hive. There are several ways of achieving this (JUnit, MRUnit, iTest or simply using scripts), but the talk introduces FitNesse as a natural language test specification, where the tests are written as stories instead as programming code. The FitNesse server translates the natural language into Java and integrates with REST or Jenkins if needed. This has several advantages, like having a wiki page with the tests written in a language that everyone can understand (or integrate PigLatin directly into that wiki page). However, natural language has its limits, like for instance, if you want to check the expected result (like the output of a Pig job alias).&lt;/p&gt;
&lt;h2&gt;Don't drown in a log ocean&lt;/h2&gt;
&lt;p&gt;Who hasn't had issues when dealing with log information? We want to know what's going on when something is wrong, but sometimes the log is too verbose (Java :P) or simply we have the log files distributed among a lot of servers. &lt;a href="http://berlinbuzzwords.de/sessions/state-open-source-logging"&gt;The State of Open Source Logging&lt;/a&gt; shows some technologies that address this problem, like fluentd, Logstash, Graylog2, ELSA, Flume or Scribe. Therefore, one work around to this problem is to centralize all log files into a Elasticsearch cluster, transforming every log line into a JSON document. The talk is focused on the &lt;strong&gt;Kibana&lt;/strong&gt; way, which is a very powerful Logstash and Elasticsearch interface.&lt;/p&gt;
&lt;h2&gt;Machine learning baby&lt;/h2&gt;
&lt;p&gt;Imagine that we have access to a lot of data, and actually we can (Twitter Public API). Now we want to do something useful with that data, like in &lt;a href="http://berlinbuzzwords.de/sessions/geospatial-event-detection-twitter-stream"&gt;Geospatial Event Detection in the Twitter Stream&lt;/a&gt;, a nice talk stating the desire of creating real actionable insights from tweet data: fires, police alerts, events, demonstrations, accidents...&lt;/p&gt;
&lt;p&gt;In order to detect those events, they grouped the tweets by location (tweet cluster) storing them in MongoDB, and then, check if the tweets from the cluster belong to the same topic (marking the group as good). Where is the machine learning? Well, marking the tweet cluster as good is not a trivial thing: the use of a machine learning tool (like Weka) can solve the problem.&lt;/p&gt;
&lt;p&gt;The machine learning tool will then make a choice (is the cluster good or bad?) based on a series of user defined rules. In this specific case, it was checking if the tweets had a common theme (n-gram overlap), sentiment (was it positive, negative? Which was the overall sentiment? What would be the sentiment strenght?), subjectivity, number of hashtags, retweet ratio, event categories, embedded links, foursquare tweets (or other kind of predefined tweets), total number of tweets in the cluster, unique locations, bad locations (airports, train stations...), among other parameters. This highlights the relevance of having an important number of quality rules, besides the tool used for performing machine learning.&lt;/p&gt;
&lt;h1&gt;Store&lt;/h1&gt;
&lt;h2&gt;Cassandra in da house&lt;/h2&gt;
&lt;p&gt;I didn't assist a lot of &lt;code&gt;Store&lt;/code&gt; talks, it was mainly about Cassandra, topic in which I am not too familiar with. &lt;img alt="On Cassandra's Evolution" src="http://berlinbuzzwords.de/sessions/cassandras-evolutions" /&gt; gave a brief explanation about Casandra's ring of nodes, remarking the difference between data distribution without and with virtual nodes. If we introduce the concept of &lt;code&gt;token&lt;/code&gt;, which belongs to a data range, so the entire data will be divided in tokens, the data distribution with virtual nodes will use more tokens per node, and smaller ones. Other concepts were explained, like repairing with and without virtual nodes, but at the end, the use of virtual nodes was more interesting: you can pick tokens from pretty much every node in the ring, instead of from some nodes of your entire cluster (this is due to the fact that the tokens are smaller), so if you have a lot of data to transfer because you are rebuilding a node, this can make the difference. There are other advantages, they allow heterogeneous nodes and the load balancing is simpler when adding new nodes.&lt;/p&gt;
&lt;p&gt;Another interesting part was the introduction to the &lt;strong&gt;Cassandra Query Language&lt;/strong&gt; (CQL3). Is kind of a "denormalized" SQL, strictly real time oriented, with no joins, no sub-queries, no aggregation and a limited ORDER BY. They also announced the replace of the Thrift transport protocol for a binary (native) one, which is asynchronous, gives server notifications for new nodes and schema changes, and is totally optimized for CQL3. Another interesting concept is the request tracing, you can trace queries, for instance, seeing which node receives the query and which nodes has the requires replicas, making debugging easier for tracing anti patterns. In &lt;img alt="Cassandra by Example" src="http://berlinbuzzwords.de/sessions/cassandra-example-data-modeling-cql3" /&gt; they extend these concepts using a Django example application called &lt;a href="https://github.com/twissandra/twissandra"&gt;Twissandra&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Scale&lt;/h1&gt;
&lt;p&gt;The most relevant talk about &lt;code&gt;scale&lt;/code&gt;, in my opinion, was &lt;a href="http://berlinbuzzwords.de/sessions/scaling-other-way-elasticsearch-miniature"&gt;Elasticsearch in Miniature&lt;/a&gt;, a nice way of presenting Elasticsearch's distributed capabilities. The cool thing is that the Elasticsearch guys installed their system in 5 Raspberry Pi, creating a test ES cluster over WiFi: this allowed them to &lt;a href="http://www.youtube.com/watch?feature=player_embedded&amp;amp;v=AA_gihv5H-Y"&gt;show us&lt;/a&gt; things like how the shards and replicas were rebalanced when the number of nodes in the cluster was changing, or what was the cluster state after destroying and creating replicas. The interesting thing is that everything went fine, having into account that this kind of demos usually tend to fail in the final presentation, and the fact that the network wasn't really reliable is a plus.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dario Blanco</dc:creator><pubDate>Tue, 30 Jul 2013 09:51:00 +0200</pubDate><guid>tag:thesoftjaguar.com,2013-07-30:posts/2013/07/30/berlin-buzzwords-2013/</guid><category>conferences</category><category>elasticsearch</category><category>solr</category><category>lucene</category><category>open-source</category><category>big-data</category><category>java</category><category>cassandra</category><category>hadoop</category><category>pig</category><category>search</category><category>store</category><category>scale</category></item><item><title>FOSDEM 2013, it is more than beer</title><link>http://thesoftjaguar.com/posts/2013/03/08/fosdem-2013/</link><description>&lt;p&gt;After my long blog hibernation, I wanted to write about &lt;em&gt;FOSDEM 2013&lt;/em&gt;, which took place in Brussels on February 2nd and 3rd, and it wasn't only for drinking beer. OK, you got me, the beer is always a motivating factor.&lt;/p&gt;
&lt;p&gt;&lt;img alt="We share a dark past" src="http://i.imgur.com/Il4kdf4.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;The Virtualization alternative&lt;/h2&gt;
&lt;p&gt;The first day I was lost, the classrooms were really crowded, and I missed the Configuration Systems Management talk because, for "security purposes", nobody else could enter in the room. There is always a B Plan, so I attended some &lt;em&gt;Xen&lt;/em&gt; talks about security using &lt;em&gt;Pygrub&lt;/em&gt; and fixed kernels, explaining different attacks that can be performed to our virtual machines, some security practices like PV VMs or the Xen Security Module, and how to virtualizate in CentOS 6. It was nice to hear about all of this, because I am not really advanced in that issue.&lt;/p&gt;
&lt;h2&gt;We should cook: Configuration Systems Management&lt;/h2&gt;
&lt;p&gt;However, Saturday afternoon was the Configuration Systems Management time. At that moment, I was aware of the overcrowding problem, so I arrived to the classroom 30 minutes earlier, and I spent the rest of the day there. From all the talks, I highlight these:&lt;/p&gt;
&lt;h3&gt;Learning to Automate&lt;/h3&gt;
&lt;p&gt;This was a continuation of what I missed that morning, but the people were explaining their knowledge about certain &lt;em&gt;DevOps&lt;/em&gt; practices, their experience with &lt;em&gt;Chef&lt;/em&gt; and &lt;em&gt;Puppet&lt;/em&gt; and how difficult can be sometimes. It was focused in how to debug your &lt;em&gt;Chef&lt;/em&gt; cookbooks, introducing other projects like &lt;a href="http://acrmp.github.com/foodcritic/"&gt;Foodcritic&lt;/a&gt; and &lt;a href="https://github.com/opscode/test-kitchen"&gt;test-kitchen&lt;/a&gt;. Of course, &lt;a href="http://www.vagrantup.com/"&gt;Vagrant&lt;/a&gt; was the recommended method for testing your young cookbooks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="We need a layered approach to Systems Management" src="http://i.imgur.com/bvFEbyM.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;Using Ruby frameworks to bring sanity to your infrastructure&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Vagrant&lt;/em&gt; was over and over again, because it is a really interesting way of creating a development environment, so I wasn't surprised about how insistent they were about its use. But the main point here was that they explained some Ruby frameworks, and more important: their thoughts about each of them. I am not a Ruby expert so it was nice to know that &lt;em&gt;Cucumber&lt;/em&gt; was not recommended at all, that &lt;em&gt;Rspec&lt;/em&gt; have &lt;em&gt;Chefspec&lt;/em&gt; and &lt;em&gt;Rspec-Puppet&lt;/em&gt; as reference in this world, and a brief introduction to &lt;em&gt;Minitest&lt;/em&gt; never hurts.&lt;/p&gt;
&lt;p&gt;However, this talk was for explaining the &lt;em&gt;Test-Kitchen&lt;/em&gt; project: useful for testing cookbooks across different operating systems, and capable of running the tests in Virtualbox or Openstack. Then you realize how complicated the testing of Configuration Management Systems can be, but possible after all.&lt;/p&gt;
&lt;p&gt;Besides all of this, other libraries were meant, like &lt;em&gt;Celulloid&lt;/em&gt;, &lt;em&gt;Bats&lt;/em&gt; (Bash automated testing system), &lt;em&gt;Chef-Workflow&lt;/em&gt;, &lt;em&gt;Berkshelf&lt;/em&gt;, &lt;em&gt;Apache Mesos&lt;/em&gt;, &lt;em&gt;Faraday&lt;/em&gt;, &lt;em&gt;Sinatra&lt;/em&gt;, &lt;em&gt;Rspec-dns&lt;/em&gt;, &lt;em&gt;Ruby-dns&lt;/em&gt; and more... That was all the libraryfest for the day.&lt;/p&gt;
&lt;h3&gt;VeeWee&lt;/h3&gt;
&lt;p&gt;I enjoyed this one because it was a surprise talk (though I didn't love why it became a surprise). It was awesome to know more about the &lt;a href="https://github.com/jedi4ever/veewee"&gt;VeeWee&lt;/a&gt; project and I definitely recommend it if you want to create almost any virtual machine in a really easy way.&lt;/p&gt;
&lt;p&gt;&lt;img alt="One does not simply choose the right kind of beer" src="http://i.imgur.com/Xrv3xC0.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;It's Python time&lt;/h2&gt;
&lt;p&gt;Sunday was the Python day, an overcrowded room as on Saturday but I could manage to pick a nice seat. The first talk was &lt;em&gt;Astonishing Python tricks&lt;/em&gt;, a short one but explaining some Python particularities and putting together several Python patterns that the community have been using. Besides this, there are other talks that I want to highlight:&lt;/p&gt;
&lt;h3&gt;Gaffer - Application deployment, monitoring and supervision made simple&lt;/h3&gt;
&lt;p&gt;I was skeptic about &lt;a href="http://gaffer.readthedocs.org/"&gt;Gaffer&lt;/a&gt;, but I am going to give it a try. At first I thought that it was like a distributed supervisor but it is more than that. I can think in a lot of uses for a company environment, and I am planning an open source project using this library.&lt;/p&gt;
&lt;h3&gt;Plone, the best python-based CMS&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://plone.org/"&gt;Plone&lt;/a&gt; seems a really mature Python CMS. The security was his strongest point (and Python :P) but the weakest one was the deployment (as expected). If you really want to deploy even the most basic application, and you are not familiar to the basic concepts (and some other not as basics), is going to be difficult. They acknowledged those problems, but I have to say that as a CMS solution seems a really interesting one, if you can overcome the drawbacks. In my case, I am a control maniac, so that's why I prefer microframeworks like &lt;em&gt;Flask&lt;/em&gt;, other not as micro as &lt;em&gt;Django&lt;/em&gt;, and I intend to flee from CMS, though sometimes are really useful for certain situations.&lt;/p&gt;
&lt;h3&gt;TDD from scratch&lt;/h3&gt;
&lt;p&gt;We love &lt;em&gt;Test Driven Development&lt;/em&gt;, and if not, you should. Sometimes is hard, really hard, to program the test, because we are lazy. But then, with the experience, you realize that if we would have programmed a test at first, everything would have been easier. This talk addressed all the benefits of that philosophy change, and what tools we can use for easing the process.&lt;/p&gt;
&lt;h3&gt;Vaurien the Chaos TCP Proxy&lt;/h3&gt;
&lt;p&gt;Based on Netflix Chaos Monkey project, &lt;a href="http://vaurien.readthedocs.org/"&gt;Vaurien&lt;/a&gt; seems an useful way for testing distributed systems (and yeah, distributed systems need a lot of testing). Having a proxy between your code and any other system, in which you can put different protocols and behaviors, seems a really good approach for it. I am currently doing some testing actions with mock, and I think that &lt;em&gt;Vaurien&lt;/em&gt; could solve them easily , so giving this project a try wouldn't be a bad idea.&lt;/p&gt;
&lt;h3&gt;Python for Humans&lt;/h3&gt;
&lt;p&gt;My favorite talk, because it was a constructive Python critic, with the open source mind as a solution to all of its problems. I don't have to add much more because the slides are easy to be &lt;a href="https://speakerdeck.com/kennethreitz/python-for-humans"&gt;found&lt;/a&gt;. By the way, I also loved the &lt;a href="http://python-guide.org"&gt;The Hitchhiker’s Guide to Python&lt;/a&gt;&lt;/a&gt; project, I think that anyone who wants to get started with Python should take a look at it.&lt;/p&gt;
&lt;h3&gt;How do event loops work in Python?&lt;/h3&gt;
&lt;p&gt;I love Asynchronous Programming, and I love how stupid I feel when I am dealing with it. In this talk, the &lt;a href="http://pyuv.readthedocs.org"&gt;pyuv&lt;/a&gt; was introduced, and it seems really promising. It can be integrated with Twisted and Tornado. Besides that, there was an explanation and comparison of the different event loop libraries.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Believe it or not, this place was an important piece of FOSDEM 2013" src="http://i.imgur.com/pWQSUZk.jpg" /&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dario Blanco</dc:creator><pubDate>Fri, 08 Mar 2013 20:02:00 +0100</pubDate><guid>tag:thesoftjaguar.com,2013-03-08:posts/2013/03/08/fosdem-2013/</guid><category>conferences</category><category>python</category><category>ruby</category><category>devops</category><category>virtualization</category><category>tdd</category><category>open-source</category></item><item><title>ApacheCon Europe 2012</title><link>http://thesoftjaguar.com/posts/2012/11/11/apachecon-europe-2012/</link><description>&lt;p&gt;On Wednesday it was ApacheCon day, lot of interesting stuff regarding Apache Lucene, Solr and Elasticsearch. I am not an expert with search engines, and this is a good start for establishing the basic concepts and understanding how some people have solved search problems in their systems.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Captain Obvious strikes again" src="https://pbs.twimg.com/media/A7Fb2aPCAAAtQy2.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;Compound Terms Query Parser for Great Shopping Experience&lt;/h2&gt;
&lt;p&gt;This &lt;a href="http://www.apachecon.eu/schedule/presentation/18/?utm_source=twitter&amp;amp;utm_medium=social&amp;amp;utm_content=79e580ba-95ca-43cb-9739-95693cf4770e"&gt;talk&lt;/a&gt; explained the problem of returning precise search results when there are tricky queries, based on &lt;a href="http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CCcQFjAA&amp;amp;url=http%3A%2F%2Flucene.sourceforge.net%2Fpapers%2Friao97.ps&amp;amp;ei=7O-fUPHRLYSh4gTA4IGQCA&amp;amp;usg=AFQjCNF3SmHeiPe-h1A48ztdx8Qx1R-r7w"&gt;Space Optimizations for Total Ranking&lt;/a&gt; paper, and a match spotting using the info explained by the client and facet counts.&lt;/p&gt;
&lt;p&gt;However, there are some queries which are almost impossible to guess, like for example &lt;code&gt;red jeans&lt;/code&gt; in a search system which supports colors, you will have red clothes matches, matches with trademarks whose name contains &lt;code&gt;red&lt;/code&gt; or &lt;code&gt;jeans&lt;/code&gt;. You can minimize the problem, and of course the client only wants exact results, not a huge list of results that he is not going to check, but at the end a nice approach will be less guessing and more freedom to the client in his queries, for instance, specifying if he want to search by color or by trademark.&lt;/p&gt;
&lt;h2&gt;Apache Mahout in context&lt;/h2&gt;
&lt;p&gt;A nice &lt;a href="http://www.apachecon.eu/schedule/presentation/1/"&gt;explanation&lt;/a&gt; of the Apache Mahout project as a data analysis tool, and use cases of what we can do with our huge load of data.&lt;/p&gt;
&lt;p&gt;A good example of this is the &lt;code&gt;Frequently Bought Together&lt;/code&gt; and &lt;code&gt;What Other Items Do Customers Buy After Viewing This Item?&lt;/code&gt; tags in an Amazon product page.&lt;/p&gt;
&lt;h2&gt;Big Search with Big Data Principles&lt;/h2&gt;
&lt;p&gt;Eric presented his &lt;a href="http://www.apachecon.eu/schedule/presentation/13/"&gt;problem&lt;/a&gt;, which consisted in millions of text documents that should be indexed, and they could have really strange formats.&lt;/p&gt;
&lt;p&gt;The solution was nice, applying technologies in which they were familiar with, and then he spoke about &lt;code&gt;Solr4&lt;/code&gt;. We should highlight that &lt;code&gt;Solr&lt;/code&gt; is becoming a noSQL database itself (&lt;code&gt;Elasticsearch&lt;/code&gt; as well), having a key/value store engine, and they used &lt;code&gt;.avro&lt;/code&gt; files for the cache, which was quite interesting.&lt;/p&gt;
&lt;p&gt;Other design choices were &lt;code&gt;ZooKeeper&lt;/code&gt; over &lt;code&gt;NFS&lt;/code&gt;, &lt;code&gt;SCP&lt;/code&gt;, etc.. for sending files between nodes (less error prone and more obvious reasons, &lt;code&gt;ZooKeeper&lt;/code&gt; is powerful), &lt;code&gt;Apache Tika&lt;/code&gt; as a pipeline, handling the errors with an error query, &lt;code&gt;Solrmeter&lt;/code&gt; for monitoring (the central monitoring using &lt;code&gt;Nagios&lt;/code&gt; with the &lt;code&gt;Solr&lt;/code&gt; plugin), a pooled environment (like for example 1 node for development, 6 for production and the rest for load testing) and a &lt;code&gt;Grim Reaper&lt;/code&gt; for restarting instances which are not working as expected.&lt;/p&gt;
&lt;p&gt;In addition, there were some problems which are still present, like the user queries analysis, done with Solrmeter or reading them in an excel file, and the cycle scavenging, which is being done with &lt;code&gt;Condor&lt;/code&gt; but not without complications.&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hoffenheim Arena looks good at night" src="http://i.imgur.com/puR3H.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;Solr 4: The SolrCloud Architecture&lt;/h2&gt;
&lt;p&gt;And it was the turn of &lt;a href="http://www.apachecon.eu/schedule/presentation/23/"&gt;showing&lt;/a&gt; how to hack a project that wasn't planned for distributed searching from the beggining, one of the biggest weaknesses in Solr.&lt;/p&gt;
&lt;p&gt;I want to highlight the use of &lt;code&gt;MurmurHash&lt;/code&gt; for assigning the documents to the different shards, &lt;code&gt;ZooKeeper&lt;/code&gt; again (in Elasticsearch this is definitely handled in a nicer way), &lt;code&gt;PeerSync&lt;/code&gt; for data replication and so on, and a last introduction to Netflix child: &lt;code&gt;Chaos Monkey&lt;/code&gt;, used for testing.&lt;/p&gt;
&lt;p&gt;SolrCloud hasn't convinced me, there are some problems when you want to scale the system, because there is a shard limit that you have to set at first, so &lt;code&gt;SolrCloud&lt;/code&gt; will assign a hash range to each shard. There are some hacks for allowing this, like the shard split (so the hash range will be reassigned). That's another problem that we have in Elasticsearch, but the shard arranging is automatic.&lt;/p&gt;
&lt;h2&gt;Personalized Search on the Largest Flash Site in America (Gilt)&lt;/h2&gt;
&lt;p&gt;Another talk regarding a &lt;a href="http://www.apachecon.eu/schedule/presentation/21/"&gt;problem&lt;/a&gt; and how a company has solved it.&lt;/p&gt;
&lt;p&gt;They were using Solr for getting only the items id (&lt;code&gt;skuId&lt;/code&gt; and &lt;code&gt;lookId&lt;/code&gt;), so Solr had only the index. Then, they enriched the search result asking the database for the rest of the params. For being able to do all of this, they created three own plugins for Solr, I didn't like this approach and in one case was a little hacky, but they know better about their own systems and at the end it was working as they expected it to work.&lt;/p&gt;
&lt;p&gt;Like with the first talk, we had the problem of the search query: the product data is not clean, it can have distractive descriptions, poorly named colors and misleading brand names. It was interesting how tricky can be the synonyms if your search engine supports this feature. At the end we have the same conclusion: maybe it is better if you leave that choice to the user for avoiding bad results.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Color names if you are a girl" src="https://pbs.twimg.com/media/A7G74L7CUAU6XLo.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;Battle of the giants: Apache Solr 4.0 vs Elasticsearch&lt;/h2&gt;
&lt;p&gt;The last &lt;a href="http://www.apachecon.eu/schedule/presentation/24/"&gt;talk&lt;/a&gt; was the one in which I put more expectations, and it didn't convince me, maybe because I was hyped.&lt;/p&gt;
&lt;p&gt;It was a brief explanation of each system, which is not bad, but some query examples would have helped a lot. Elasticsearch seemed really promising with the prospective search, nested objects, moving shard and replicas, more indices storage options, index structure and not needing to reload the config. And that's what Solr doesn't have.&lt;/p&gt;
&lt;p&gt;There were also some features supported by both and well, Solr supports multilingual data handling, but is not enough. I really want to try both systems but right now, from the beginning, &lt;code&gt;Elasticsearch&lt;/code&gt; is more interesting for me, especially if I want to have a distributed search environment.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dario Blanco</dc:creator><pubDate>Sun, 11 Nov 2012 20:44:00 +0100</pubDate><guid>tag:thesoftjaguar.com,2012-11-11:posts/2012/11/11/apachecon-europe-2012/</guid><category>conferences</category><category>elasticsearch</category><category>solr</category><category>lucene</category><category>open-source</category><category>big-data</category><category>java</category></item></channel></rss>